{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOfuX3+DDw9sFSx0Ch0LfBU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"fF9vrB9U9wl0","executionInfo":{"status":"ok","timestamp":1713855988531,"user_tz":-480,"elapsed":422,"user":{"displayName":"David Mak","userId":"16330670194757710136"}}},"outputs":[],"source":["import numpy as np\n","import math\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","torch.manual_seed(0)\n","\n","from tqdm.notebook import trange\n","##from tqdm import tqdm\n","import random\n","\n","class ConnectFour:\n","    def __init__(self):\n","        self.row_count = 6\n","        self.column_count = 7\n","        self.action_size = self.column_count\n","        self.in_a_row = 4\n","\n","    def get_initial_state(self):\n","        return np.zeros((self.row_count, self.column_count))\n","\n","    def get_next_state(self, state, action, player):\n","        ### the top row is index zero,\n","        ### so the place of action is the highest row of the action column\n","        row = np.max(np.where(state[:, action] == 0))\n","        state[row, action] = player\n","        return state\n","\n","    def get_valid_moves(self, state):\n","        ### return all column index where row 0 is empty\n","        return (state[0] == 0).astype(np.uint8)\n","\n","    def check_win(self, state, action):\n","        if action == None:\n","            return False\n","\n","        row = np.min(np.where(state[:, action] != 0))\n","        column = action\n","        player = state[row][column]\n","\n","        def count(offset_row, offset_column):\n","            for i in range(1, self.in_a_row):\n","                r = row + offset_row * i\n","                c = action + offset_column * i\n","                if (\n","                    r < 0\n","                    or r >= self.row_count\n","                    or c < 0\n","                    or c >= self.column_count\n","                    or state[r][c] != player\n","                ):\n","                    return i - 1\n","            return self.in_a_row - 1\n","\n","        return (\n","            count(1, 0) >= self.in_a_row - 1 # vertical\n","            or (count(0, 1) + count(0, -1)) >= self.in_a_row - 1 # horizontal\n","            or (count(1, 1) + count(-1, -1)) >= self.in_a_row - 1 # top left diagonal\n","            or (count(1, -1) + count(-1, 1)) >= self.in_a_row - 1 # top right diagonal\n","        )\n","\n","    def get_value_and_terminated(self, state, action):\n","        if self.check_win(state, action):\n","            return 1, True\n","        if np.sum(self.get_valid_moves(state)) == 0:\n","            return 0, True\n","        return 0, False\n","\n","    def get_opponent(self, player):\n","        return -player\n","\n","    def get_opponent_value(self, value):\n","        return -value\n","\n","    def change_perspective(self, state, player):\n","        return state * player\n","\n","    def get_encoded_state(self, state):\n","        encoded_state = np.stack(\n","            (state == -1, state == 0, state == 1)\n","        ).astype(np.float32)\n","\n","        return encoded_state\n","\n","class ResNet(nn.Module):\n","    def __init__(self, game, num_resBlocks, num_hidden, device):\n","        super().__init__()\n","        self.device = device\n","        self.startBlock = nn.Sequential(\n","            nn.Conv2d(3, num_hidden, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(num_hidden),\n","            nn.ReLU()\n","        )\n","\n","        self.backBone = nn.ModuleList(\n","            [ResBlock(num_hidden) for i in range(num_resBlocks)]\n","        )\n","\n","        self.policyHead = nn.Sequential(\n","            nn.Conv2d(num_hidden, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(32 * game.row_count * game.column_count, game.action_size)\n","        )\n","\n","        self.valueHead = nn.Sequential(\n","            nn.Conv2d(num_hidden, 3, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(3),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(3 * game.row_count * game.column_count, 1),\n","            nn.Tanh()\n","        )\n","        self.to(device)\n","\n","    def forward(self, x):\n","        x = self.startBlock(x)\n","        for resBlock in self.backBone:\n","            x = resBlock(x)\n","        policy = self.policyHead(x)\n","        value = self.valueHead(x)\n","        return policy, value\n","\n","class ResBlock(nn.Module):\n","    def __init__(self, num_hidden):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(num_hidden)\n","        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(num_hidden)\n","\n","    def forward(self, x):\n","        residual = x\n","        x = F.relu(self.bn1(self.conv1(x)))\n","        x = self.bn2(self.conv2(x))\n","        x += residual\n","        x = F.relu(x)\n","        return x\n","\n","class Node:\n","    def __init__(self, game, args, state, parent=None, action_taken=None, prior=0, visit_count=0):\n","        self.game = game\n","        self.args = args\n","        self.state = state\n","        self.parent = parent\n","        self.action_taken = action_taken\n","\n","        self.children = []\n","        self.prior = prior\n","\n","        self.visit_count = visit_count\n","        self.value_sum = 0\n","\n","    def is_fully_expanded(self):\n","        return len(self.children) > 0\n","\n","    def select(self):\n","        best_child = None\n","        best_ucb = -np.inf\n","\n","        for child in self.children:\n","            ucb = self.get_ucb(child)\n","            if ucb > best_ucb:\n","                best_child = child\n","                best_ucb = ucb\n","\n","        return best_child\n","\n","    def get_ucb(self, child):\n","        if child.visit_count == 0:\n","            q_value = 0\n","        else:\n","            q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n","        return q_value + self.args['C'] * (math.sqrt(self.visit_count) / (child.visit_count + 1)) * child.prior\n","\n","    def expand(self, policy):\n","        for action, prob in enumerate(policy):\n","            if prob > 0:\n","                child_state = self.state.copy()\n","                child_state = self.game.get_next_state(child_state, action, 1)\n","                child_state = self.game.change_perspective(child_state, player=-1)\n","\n","                child = Node(self.game, self.args, child_state, self, action, prob)\n","                self.children.append(child)\n","\n","        return child\n","\n","\n","    def backpropagate(self, value):\n","        self.value_sum += value\n","        self.visit_count += 1\n","\n","        value = self.game.get_opponent_value(value)\n","        if self.parent is not None:\n","            self.parent.backpropagate(value)\n","\n","class MCTS:\n","    def __init__(self, game, args, model):\n","        self.game = game\n","        self.args = args\n","        self.model = model\n","\n","    @torch.no_grad()\n","    def search(self, state):\n","        root = Node(self.game, self.args, state, visit_count=1)\n","\n","        policy, _ = self.model(\n","            torch.tensor(self.game.get_encoded_state(state), device=self.model.device).unsqueeze(0)\n","        )\n","        policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n","        policy = (1 - self.args['dirichlet_epsilon']) * policy + self.args['dirichlet_epsilon'] \\\n","            * np.random.dirichlet([self.args['dirichlet_alpha']] * self.game.action_size)\n","\n","        valid_moves = self.game.get_valid_moves(state)\n","        policy *= valid_moves\n","        policy /= np.sum(policy)\n","        root.expand(policy)\n","\n","        for search in range(self.args['num_searches']):\n","            node = root\n","\n","            while node.is_fully_expanded():\n","                node = node.select()\n","\n","            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n","            value = self.game.get_opponent_value(value)\n","\n","            if not is_terminal:\n","                policy, value = self.model(\n","                    torch.tensor(self.game.get_encoded_state(node.state), device=self.model.device).unsqueeze(0)\n","                )\n","                policy = torch.softmax(policy, axis=1).squeeze(0).cpu().numpy()\n","                valid_moves = self.game.get_valid_moves(node.state)\n","                policy *= valid_moves\n","                policy /= np.sum(policy)\n","\n","                value = value.item()\n","\n","                node.expand(policy)\n","\n","            node.backpropagate(value)\n","\n","\n","        action_probs = np.zeros(self.game.action_size)\n","        for child in root.children:\n","            action_probs[child.action_taken] = child.visit_count\n","        action_probs /= np.sum(action_probs)\n","        return action_probs\n","\n","class AlphaZero:\n","    def __init__(self, model, optimizer, game, args):\n","        self.model = model\n","        self.optimizer = optimizer\n","        self.game = game\n","        self.args = args\n","        self.mcts = MCTS(game, args, model)\n","\n","    def selfPlay(self):\n","        memory = []\n","        player = 1\n","        state = self.game.get_initial_state()\n","\n","        while True:\n","            neutral_state = self.game.change_perspective(state, player)\n","            action_probs = self.mcts.search(neutral_state)\n","\n","            memory.append((neutral_state, action_probs, player))\n","\n","\n","            temperature_action_probs = action_probs ** (1 / self.args['temperature'])\n","            temperature_action_probs /= np.sum(temperature_action_probs)\n","            action = np.random.choice(self.game.action_size, p=temperature_action_probs)\n","\n","            state = self.game.get_next_state(state, action, player)\n","\n","            value, is_terminal = self.game.get_value_and_terminated(state, action)\n","\n","            if is_terminal:\n","                returnMemory = []\n","                for hist_neutral_state, hist_action_probs, hist_player in memory:\n","                    hist_outcome = value if hist_player == player else self.game.get_opponent_value(value)\n","                    returnMemory.append((\n","                        self.game.get_encoded_state(hist_neutral_state),\n","                        hist_action_probs,\n","                        hist_outcome\n","                    ))\n","                return returnMemory\n","\n","            player = self.game.get_opponent(player)\n","\n","\n","\n","    def train(self, memory):\n","        random.shuffle(memory)\n","        for batchIdx in range(0, len(memory), self.args['batch_size']):\n","            sample = memory[batchIdx:min(len(memory) - 1, batchIdx + self.args['batch_size'])] # Change to memory[batchIdx:batchIdx+self.args['batch_size']] in case of an error\n","            state, policy_targets, value_targets = zip(*sample)\n","\n","            state, policy_targets, value_targets = np.array(state), np.array(policy_targets), np.array(value_targets).reshape(-1, 1)\n","\n","            state = torch.tensor(state, dtype=torch.float32, device=self.model.device)\n","            policy_targets = torch.tensor(policy_targets, dtype=torch.float32, device=self.model.device)\n","            value_targets = torch.tensor(value_targets, dtype=torch.float32, device=self.model.device)\n","\n","            out_policy, out_value = self.model(state)\n","\n","            policy_loss = F.cross_entropy(out_policy, policy_targets)\n","            value_loss = F.mse_loss(out_value, value_targets)\n","            loss = policy_loss + value_loss\n","\n","            self.optimizer.zero_grad() # change to self.optimizer\n","            loss.backward()\n","            self.optimizer.step() # change to self.optimizer\n","\n","    def learn(self):\n","        for iteration in range(self.args['num_iterations']):\n","            memory = []\n","\n","            self.model.eval()\n","            for selfPlay_iteration in trange(self.args['num_selfPlay_iterations']):\n","                memory += self.selfPlay()\n","\n","            self.model.train()\n","            for epoch in trange(self.args['num_epochs']):\n","                self.train(memory)\n","\n","            torch.save(self.model.state_dict(), f\"model_{iteration}.pt\")\n","            torch.save(self.optimizer.state_dict(), f\"optimizer_{iteration}.pt\")\n","class Node_Orig:\n","    def __init__(self, game, args, state, parent=None, action_taken=None):\n","        self.game = game\n","        self.args = args\n","        self.state = state\n","        self.parent = parent\n","        self.action_taken = action_taken\n","\n","        self.children = []\n","        self.expandable_moves = game.get_valid_moves(state)\n","\n","        self.visit_count = 0\n","        self.value_sum = 0\n","\n","    def is_fully_expanded(self):\n","        return np.sum(self.expandable_moves) == 0 and len(self.children) > 0\n","\n","    def select(self):\n","        best_child = None\n","        best_ucb = -np.inf\n","\n","        for child in self.children:\n","            ucb = self.get_ucb(child)\n","            if ucb > best_ucb:\n","                best_child = child\n","                best_ucb = ucb\n","\n","        return best_child\n","\n","    def get_ucb(self, child):\n","        q_value = 1 - ((child.value_sum / child.visit_count) + 1) / 2\n","        return q_value + self.args['C'] * math.sqrt(math.log(self.visit_count) / child.visit_count)\n","\n","    def expand(self):\n","        action = np.random.choice(np.where(self.expandable_moves == 1)[0])\n","        self.expandable_moves[action] = 0\n","\n","        child_state = self.state.copy()\n","        child_state = self.game.get_next_state(child_state, action, 1)\n","        child_state = self.game.change_perspective(child_state, player=-1)\n","\n","        child = Node_Orig(self.game, self.args, child_state, self, action)\n","        self.children.append(child)\n","        return child\n","\n","    def simulate(self):\n","        value, is_terminal = self.game.get_value_and_terminated(self.state, self.action_taken)\n","        value = self.game.get_opponent_value(value)\n","\n","        if is_terminal:\n","            return value\n","\n","        rollout_state = self.state.copy()\n","        rollout_player = 1\n","        while True:\n","\n","            valid_moves = self.game.get_valid_moves(rollout_state)\n","\n","            # heuristic update\n","\n","            #action = np.random.choice(np.where(valid_moves == 1)[0])\n","            #print(\"player: \", rollout_player)\n","            #print(\"initial rollout_state: \", rollout_state)\n","\n","            # always select winning moves if available\n","            r_player = rollout_player\n","            my_winning_action= np.zeros(self.game.action_size)\n","            for action in np.where(valid_moves == 1)[0]:\n","                my_win_state = rollout_state.copy()\n","                my_win_state = self.game.get_next_state(my_win_state, action, r_player)\n","                value, is_terminal = self.game.get_value_and_terminated(my_win_state, action)\n","                if value == 1:\n","                    my_winning_action[action] = 1\n","\n","            # always block opponent's winning moves if we have no winning move\n","            r_player = self.game.get_opponent(r_player)\n","            op_winning_action= np.zeros(self.game.action_size)\n","            for action in np.where(valid_moves == 1)[0]:\n","                op_win_state = rollout_state.copy()\n","                op_win_state = self.game.get_next_state(op_win_state, action, r_player)\n","                value, is_terminal = self.game.get_value_and_terminated(op_win_state, action)\n","                if value == 1:\n","                    op_winning_action[action] = 1\n","\n","            #print(\"current player winning move: \", my_winning_action)\n","            #print(\"opponent player winning move: \", op_winning_action)\n","            #print(\"valid moves: \", valid_moves)\n","\n","            # if no winning moves or opponent winning moves, use back the valid moves policy\n","            if 1 in my_winning_action:\n","                policy_fn = my_winning_action\n","            elif 1 in op_winning_action:\n","                policy_fn = op_winning_action\n","            else:\n","                policy_fn = valid_moves\n","\n","            #print(\"final policy function: \", policy_fn)\n","\n","            action = np.random.choice(np.where(policy_fn == 1)[0])\n","\n","            rollout_state = self.game.get_next_state(rollout_state, action, rollout_player)\n","            value, is_terminal = self.game.get_value_and_terminated(rollout_state, action)\n","            #print(\"rollout state after action: \", rollout_state)\n","            #print(\"player who took the action: \", rollout_player)\n","            if is_terminal:\n","                if rollout_player == -1:\n","                    value = self.game.get_opponent_value(value)\n","                return value\n","\n","            rollout_player = self.game.get_opponent(rollout_player)\n","\n","    def backpropagate(self, value):\n","        self.value_sum += value\n","        self.visit_count += 1\n","\n","        value = self.game.get_opponent_value(value)\n","        if self.parent is not None:\n","            self.parent.backpropagate(value)\n","\n","class MCTS_Orig:\n","    def __init__(self, game, args):\n","        self.game = game\n","        self.args = args\n","\n","    def search(self, state):\n","        root = Node_Orig(self.game, self.args, state)\n","\n","        for search in range(self.args['num_searches']):\n","            node = root\n","\n","            while node.is_fully_expanded():\n","                node = node.select()\n","\n","            value, is_terminal = self.game.get_value_and_terminated(node.state, node.action_taken)\n","            value = self.game.get_opponent_value(value)\n","\n","            if not is_terminal:\n","                node = node.expand()\n","                value = node.simulate()\n","\n","            node.backpropagate(value)\n","\n","\n","        action_probs = np.zeros(self.game.action_size)\n","        for child in root.children:\n","            action_probs[child.action_taken] = child.visit_count\n","        action_probs /= np.sum(action_probs)\n","        return action_probs\n","\n","def main():\n","    game = ConnectFour()\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    player = 1\n","\n","    args = {\n","        'C': 2,\n","        'num_searches': 100,\n","        'dirichlet_epsilon': 0.,\n","        'dirichlet_alpha': 0.3\n","    }\n","\n","    args_orig = {\n","        'C': 1.41,\n","        'num_searches': 10000\n","    }\n","\n","\n","    #model = ResNet(game, 9, 128, device=device)\n","    #model.load_state_dict(torch.load('model_7_ConnectFour.pt', map_location=device))\n","    #model.eval()\n","\n","    #below can move to a separate function\n","    #mcts = MCTS(game, args, model)\n","    mcts_orig = MCTS_Orig(game, args_orig)\n","    state = game.get_initial_state()\n","    while True:\n","        print(state)\n","\n","        if player == 1:\n","            start = time.time()\n","            neutral_state = game.change_perspective(state, player)\n","            mcts_probs = mcts_orig.search(neutral_state)\n","            action = np.argmax(mcts_probs)\n","            runtime = time.time()-start\n","            print(runtime, \" second\")\n","        else:\n","            # neutral_state = game.change_perspective(state, player)\n","            # mcts_probs = mcts.search(neutral_state)\n","            # action = np.argmax(mcts_probs)\n","            action = int(input())\n","        state = game.get_next_state(state, action, player)\n","\n","        value, is_terminal = game.get_value_and_terminated(state, action)\n","\n","        if is_terminal:\n","            print(state)\n","            if value == 1:\n","                print(player, \"won\")\n","                return_winner = player\n","            else:\n","                print(\"draw\")\n","                return_winner = 0\n","            break\n","\n","        player = game.get_opponent(player)\n","\n","def test():\n","\n","    tictactoe = TicTacToe()\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    state = tictactoe.get_initial_state()\n","    state = tictactoe.get_next_state(state, 2, -1)\n","    state = tictactoe.get_next_state(state, 4, -1)\n","    state = tictactoe.get_next_state(state, 6, 1)\n","    state = tictactoe.get_next_state(state, 8, 1)\n","\n","    print(state)\n","\n","    encoded_state = tictactoe.get_encoded_state(state)\n","\n","    print(encoded_state)\n","\n","    tensor_state = torch.tensor(encoded_state, device=device).unsqueeze(0)\n","\n","    model = ResNet(tictactoe, 4, 64, device=device)\n","    model.load_state_dict(torch.load('model_2.pt', map_location=device))\n","\n","    model.eval()\n","\n","    policy, value = model(tensor_state)\n","    value = value.item()\n","    policy = torch.softmax(policy, axis=1).squeeze(0).detach().cpu().numpy()\n","\n","    print(value, policy)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"89oGVCf8mZHk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["  ### play the actual game\n","  main()"],"metadata":{"id":"pqW64Q3t-PCS","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1713862578184,"user_tz":-480,"elapsed":5432842,"user":{"displayName":"David Mak","userId":"16330670194757710136"}},"outputId":"7e426c1a-9345-42e8-f0de-ceeb2f8ccab7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0.]]\n","166.15748167037964  second\n","[[0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0.]]\n","3\n","[[ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0. -1.  0.  0.  0.]\n"," [ 0.  0.  0.  1.  0.  0.  0.]]\n","131.1839156150818  second\n","[[ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  1.  0.  0.  0.]\n"," [ 0.  0.  0. -1.  0.  0.  0.]\n"," [ 0.  0.  0.  1.  0.  0.  0.]]\n","3\n","[[ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0. -1.  0.  0.  0.]\n"," [ 0.  0.  0.  1.  0.  0.  0.]\n"," [ 0.  0.  0. -1.  0.  0.  0.]\n"," [ 0.  0.  0.  1.  0.  0.  0.]]\n","100.2813766002655  second\n","[[ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0. -1.  0.  0.  0.]\n"," [ 0.  0.  0.  1.  0.  0.  0.]\n"," [ 0.  0.  0. -1.  0.  0.  0.]\n"," [ 0.  0.  1.  1.  0.  0.  0.]]\n","4\n","[[ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0. -1.  0.  0.  0.]\n"," [ 0.  0.  0.  1.  0.  0.  0.]\n"," [ 0.  0.  0. -1.  0.  0.  0.]\n"," [ 0.  0.  1.  1. -1.  0.  0.]]\n","100.9636561870575  second\n","[[ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0. -1.  0.  0.  0.]\n"," [ 0.  0.  0.  1.  0.  0.  0.]\n"," [ 0.  0.  0. -1.  1.  0.  0.]\n"," [ 0.  0.  1.  1. -1.  0.  0.]]\n","4\n","[[ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0. -1.  0.  0.  0.]\n"," [ 0.  0.  0.  1. -1.  0.  0.]\n"," [ 0.  0.  0. -1.  1.  0.  0.]\n"," [ 0.  0.  1.  1. -1.  0.  0.]]\n","80.93537425994873  second\n","[[ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0. -1.  1.  0.  0.]\n"," [ 0.  0.  0.  1. -1.  0.  0.]\n"," [ 0.  0.  0. -1.  1.  0.  0.]\n"," [ 0.  0.  1.  1. -1.  0.  0.]]\n","1\n","[[ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0. -1.  1.  0.  0.]\n"," [ 0.  0.  0.  1. -1.  0.  0.]\n"," [ 0.  0.  0. -1.  1.  0.  0.]\n"," [ 0. -1.  1.  1. -1.  0.  0.]]\n","78.32559466362  second\n","[[ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0. -1.  1.  0.  0.]\n"," [ 0.  0.  0.  1. -1.  0.  0.]\n"," [ 0.  0.  1. -1.  1.  0.  0.]\n"," [ 0. -1.  1.  1. -1.  0.  0.]]\n","2\n","[[ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0. -1.  1.  0.  0.]\n"," [ 0.  0. -1.  1. -1.  0.  0.]\n"," [ 0.  0.  1. -1.  1.  0.  0.]\n"," [ 0. -1.  1.  1. -1.  0.  0.]]\n","44.998461961746216  second\n","[[ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0. -1.  1.  0.  0.]\n"," [ 0.  0. -1.  1. -1.  0.  0.]\n"," [ 0.  1.  1. -1.  1.  0.  0.]\n"," [ 0. -1.  1.  1. -1.  0.  0.]]\n","2\n","[[ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0. -1. -1.  1.  0.  0.]\n"," [ 0.  0. -1.  1. -1.  0.  0.]\n"," [ 0.  1.  1. -1.  1.  0.  0.]\n"," [ 0. -1.  1.  1. -1.  0.  0.]]\n","38.21299195289612  second\n","[[ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  1.  0.  0.  0.  0.]\n"," [ 0.  0. -1. -1.  1.  0.  0.]\n"," [ 0.  0. -1.  1. -1.  0.  0.]\n"," [ 0.  1.  1. -1.  1.  0.  0.]\n"," [ 0. -1.  1.  1. -1.  0.  0.]]\n","6\n","[[ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  1.  0.  0.  0.  0.]\n"," [ 0.  0. -1. -1.  1.  0.  0.]\n"," [ 0.  0. -1.  1. -1.  0.  0.]\n"," [ 0.  1.  1. -1.  1.  0.  0.]\n"," [ 0. -1.  1.  1. -1.  0. -1.]]\n","33.48433542251587  second\n","[[ 0.  0.  0.  0.  0.  0.  0.]\n"," [ 0.  0.  1.  1.  0.  0.  0.]\n"," [ 0.  0. -1. -1.  1.  0.  0.]\n"," [ 0.  0. -1.  1. -1.  0.  0.]\n"," [ 0.  1.  1. -1.  1.  0.  0.]\n"," [ 0. -1.  1.  1. -1.  0. -1.]]\n","3\n","[[ 0.  0.  0. -1.  0.  0.  0.]\n"," [ 0.  0.  1.  1.  0.  0.  0.]\n"," [ 0.  0. -1. -1.  1.  0.  0.]\n"," [ 0.  0. -1.  1. -1.  0.  0.]\n"," [ 0.  1.  1. -1.  1.  0.  0.]\n"," [ 0. -1.  1.  1. -1.  0. -1.]]\n","27.246013879776  second\n","[[ 0.  0.  0. -1.  0.  0.  0.]\n"," [ 0.  0.  1.  1.  1.  0.  0.]\n"," [ 0.  0. -1. -1.  1.  0.  0.]\n"," [ 0.  0. -1.  1. -1.  0.  0.]\n"," [ 0.  1.  1. -1.  1.  0.  0.]\n"," [ 0. -1.  1.  1. -1.  0. -1.]]\n","4\n","[[ 0.  0.  0. -1. -1.  0.  0.]\n"," [ 0.  0.  1.  1.  1.  0.  0.]\n"," [ 0.  0. -1. -1.  1.  0.  0.]\n"," [ 0.  0. -1.  1. -1.  0.  0.]\n"," [ 0.  1.  1. -1.  1.  0.  0.]\n"," [ 0. -1.  1.  1. -1.  0. -1.]]\n","16.571486949920654  second\n","[[ 0.  0.  0. -1. -1.  0.  0.]\n"," [ 0.  0.  1.  1.  1.  0.  0.]\n"," [ 0.  0. -1. -1.  1.  0.  0.]\n"," [ 0.  0. -1.  1. -1.  0.  0.]\n"," [ 0.  1.  1. -1.  1.  0.  0.]\n"," [ 1. -1.  1.  1. -1.  0. -1.]]\n","6\n","[[ 0.  0.  0. -1. -1.  0.  0.]\n"," [ 0.  0.  1.  1.  1.  0.  0.]\n"," [ 0.  0. -1. -1.  1.  0.  0.]\n"," [ 0.  0. -1.  1. -1.  0.  0.]\n"," [ 0.  1.  1. -1.  1.  0. -1.]\n"," [ 1. -1.  1.  1. -1.  0. -1.]]\n","13.247795581817627  second\n","[[ 0.  0.  0. -1. -1.  0.  0.]\n"," [ 0.  0.  1.  1.  1.  0.  0.]\n"," [ 0.  0. -1. -1.  1.  0.  0.]\n"," [ 0.  0. -1.  1. -1.  0.  0.]\n"," [ 1.  1.  1. -1.  1.  0. -1.]\n"," [ 1. -1.  1.  1. -1.  0. -1.]]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-ae864257e428>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### play the actual game\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-91743fac0cb0>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;31m# mcts_probs = mcts.search(neutral_state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0;31m# action = np.argmax(mcts_probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]},{"cell_type":"code","source":[],"metadata":{"id":"EgzSaetK-XSy"},"execution_count":null,"outputs":[]}]}